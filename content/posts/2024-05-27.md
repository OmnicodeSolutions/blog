+++
title = "2024-05-27"
date = 2024-05-27

[taxonomies]
authors = ["Lu√≠sa Coelho"]
+++

---

Today is Monday, day 148 of 366 (40.43%). We're in week 22 of 52 (42.30%).

## Planning

What I am going to do today:

- [ ] Finish internship report with LLMs

## Activities executed

Today I almost finished the report using LLMs. I found this [LangChain tutorial](https://blog.langchain.dev/tutorial-chatgpt-over-your-data/) on how to use ChatGPT over your own data, and it helped me realize (after a lot of reading) that the Chroma completion is not an embedding, it's a vector database. This means all I had to do was give it to ChatGPT and ask for the report. The problem is: there are too many tokens in the response. When I tried to run [`query_data`](https://github.com/OmnicodeSolutions/blog/blob/summarize_llm/query_data.py), I got:

```bash
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-VNt9wqGeLXaFB7IsuMgW42Qi on tokens per min (TPM): Limit 30000, Requested 40539. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
```

In this example, I was using `gpt-4o` as my model. I went to the link to find a model with a larger token limit and it says that `gpt-3.5-turbo-instruct` and `gpt-3.5-turbo-instruct-0914` have 90,000 TPM. So naturally, I just plugged the first one into the code hoping it would work, but I got another error:

```bash
openai.NotFoundError: Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
```

I couldn't find LangChain's supported types anywhere and also no official documentation on how to use `v1/completions` instead of `v1/chat/completions`. I'll try looking for it a bit more tomorrow, but I might just give up on LangChain and use OpenAI directly.
