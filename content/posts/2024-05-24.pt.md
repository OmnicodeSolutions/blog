+++
title = "2024-05-24"
date = 2024-05-24

[taxonomies]
authors = ["Luísa Coelho"]
+++

---

Hoje é Sexta, dia 145 de 366 (39.61%). Estamos na semana 21 de 52 (40.38%).

## Planejamento

O que vou fazer hoje:

- [x] [Fazer mudanças sugeridas no script de tradução](https://github.com/OmnicodeSolutions/blog/pull/184#pullrequestreview-2074109013)
- [ ] Terminar o bot que vai me ajudar com o relatório

## Atividades executadas

Hoje fiz algumas alterações no script de tradução a pedido do Mauricio; ele também fez algumas mudanças e está praticamente pronto.

Também tentei finalizar o relatório usando LLMs, mas fiquei um pouco perdido no processo. Fiquei confuso sobre se realmente precisava de um embedding para isso, já que não estou fazendo um sistema de busca/recomendação, e comecei a pesquisar mais para ver se encontrava algo semelhante ao que estou tentando fazer. Encontrei [esta documentação do Langchain](https://python.langchain.com/v0.2/docs/tutorials/graph/) sobre aplicações de resposta a perguntas com bancos de dados gráficos e achei que seria uma boa forma de fazer o que quero, mas não faço ideia de como criar as arestas.

Depois de pensar no que realmente preciso por um tempo, pedi ajuda ao ChatGPT e ele me deu um código de exemplo que não funciona e um passo a passo:

```
Etapa 1: Extrair Dados do Blog Zola
Primeiro, extraia o conteúdo das suas postagens no blog. Supondo que você tenha acesso aos arquivos markdown, você pode lê-los em seu script. Se eles estiverem hospedados online, pode ser necessário fazer a raspagem dos dados ou usar uma API, se disponível.

Etapa 2: Pré-processar os Dados
Pré-processe os dados extraídos para torná-los adequados para consulta e resumo. Isso pode incluir a limpeza do texto, organizá-lo por data e separar diferentes seções das entradas.

Etapa 3: Configurar LangChain e ChromaDB
O LangChain ajudará você a gerenciar a interação com modelos de linguagem, e o ChromaDB pode ser usado para armazenar e consultar o conteúdo do seu blog. Você pode indexar suas entradas diárias no ChromaDB para permitir uma recuperação eficiente.

Etapa 4: Consultar os Dados e Gerar Seções do Relatório
Use o GPT-4 da OpenAI para gerar o conteúdo para cada seção do relatório. Você pode criar prompts que resumem as atividades, metodologias e conclusões com base nas suas entradas do blog.
```

As três primeiras etapas já estão praticamente cobertas no meu código atual, e a etapa 4 é exatamente onde parei ontem e o que me causou essa confusão. No entanto, aparentemente posso usar os embeddings em vez de um banco de dados, o que é bom; só preciso descobrir como fazer isso e não acho que o código do ChatGPT me ajude nesse sentido.
