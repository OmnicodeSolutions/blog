+++
title = "2024-05-23"
date = 2024-05-23

[taxonomies]
authors = ["Lu√≠sa Coelho"]
+++

---

Today is Thursday, day 144 of 366 (39.34%). We're in week 21 of 52 (40.38%).

## Planning

What I am going to do today:

- [x] Finish LLMs studies
- [x] Start working on the AI to summarize the blog posts

## Activities executed

Today I watched a video about ChromaDB and read an article about LLMs (see the Readings section).

I also started putting everything I learned into practice by working on using AI to help me with the internship report. I began by trying to use Langchain to ask ChatGPT to summarize one of the blog posts. I found [this official tutorial](https://python.langchain.com/v0.1/docs/use_cases/summarization/), so it wasn't difficult. You can check the code [here](https://github.com/OmnicodeSolutions/blog/commit/39e54534b230799e2cdf1958a73fe3a9e7a75a43), but here's the step-by-step (assuming you already have your key in the `.env` file):

1. Create a loader for the file you want to summarize. In my case, it was a markdown file, so I used `UnstructuredMarkdownLoader`:
```python
markdown_path = "content/posts/2024-04-16.md"
loader = UnstructuredMarkdownLoader(markdown_path)
```

2. Define the prompt, which is what you want the AI to do (write a concise summary), what you will provide it (`text`), and what you want in return (`CONCISE SUMMARY`):

```python
prompt_template = """Write a concise summary of the following:
"{text}"
CONCISE SUMMARY:"""
prompt = PromptTemplate.from_template(prompt_template)
```

3. Define the LLM chain:

```python
llm = ChatOpenAI(temperature=0, model_name="gpt-4")
llm_chain = LLMChain(llm=llm, prompt=prompt)
```

4. Load the file, extract the content, and run the LLM chain:

```python
docs = loader.load()
content = docs[0].page_content # You can check how the markdown file is organized by printing `docs`

response = llm_chain.run(content)
print(response)
```

When I got this to work, I started thinking about how to get all the files from the [posts folder](https://github.com/OmnicodeSolutions/blog/tree/main/content/posts) to create an embedding of them. I decided I only need the files in English, since I can always use the script I made to translate them. So here's what I ended up doing:

```python
import os

folder_path = "content/posts"
files = os.listdir(folder_path) # Returns a list with the name of each file in the path
en_files = [file for file in files if '.pt' not in file and '_index' not in file]

for file in en_files:
    markdown_path = f"{folder_path}{file}"
    loader = UnstructuredMarkdownLoader(markdown_path)

    docs = loader.load()
    content = docs[0].page_content
```

Then, all I had to do was create the embedding. This was a bit more complicated, but I did something very similar to [this video](https://www.youtube.com/watch?v=QSW2L8dkaZk) from yesterday. You can find the final code [here](https://github.com/OmnicodeSolutions/blog/blob/52ef1e547b06d6befc7667ff84640cb19ff54c53/summarize.py#L1C1-L47C2).

Tomorrow, I need to figure out how to use Langchain to pass this embedding to ChatGPT so that it can use it as a base to make the report.

## Learnings

Today I learned that I actually didn't have to use ChromaDB directly, since Langchain is integrated with it and you can use native functions while it runs ChromaDB for you in the background. But since I had already made the embedding by this point, I decided to stick with it. You can find more about Langchain and ChromaDB [here](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/), including how to pass an existing Chroma Client into Langchain.

## Readings

1. [Getting Started with ChromaDB - Lowest Learning Curve Vector Database For Semantic Search](https://www.youtube.com/watch?v=QSW2L8dkaZk):
   easy tutorial on ChromaDB.

2. [A Gentle Intro to Chaining LLMs, Agents, and utils via Langchain](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-Langchain-16cd385fca81):
   really nice article, explains very well how to create and use Agents and chains.
